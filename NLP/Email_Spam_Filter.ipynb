{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Email Spam Filter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyn-bhBDKO4P",
        "colab_type": "text"
      },
      "source": [
        "# Download Data and Pretrained Word Embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUANY4XtO21u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p data/spam_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaqpMpuHO2vC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Get dataset and save in local folder ##\n",
        "\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2\n",
        "\n",
        "!tar xvjf 20030228_easy_ham_2.tar.bz2\n",
        "!tar xvjf 20030228_easy_ham.tar.bz2\n",
        "!tar xvjf 20030228_hard_ham.tar.bz2    \n",
        "!tar xvjf 20030228_spam.tar.bz2\n",
        "!tar xvjf 20050311_spam_2.tar.bz2\n",
        "\n",
        "!mv easy_ham data/spam_data  \n",
        "!mv easy_ham_2 data/spam_data\n",
        "!mv hard_ham data/spam_data\n",
        "!mv spam data/spam_data\n",
        "!mv spam_2 data/spam_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqXqV3uNKTAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Download and unzip the GloVe embedding ##\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "!python -m gensim.scripts.glove2word2vec -i glove.6B.300d.txt -o glove.6B.300d.word2vec.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Yb3qNBUPesx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Import NLTK packages ##\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRP9_1TKJgz8",
        "colab_type": "text"
      },
      "source": [
        "# Extract email content and create training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2lesDxPJgz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Specify path to email data ##\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import email\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "path = 'data/spam_data/'\n",
        "\n",
        "easy_ham_paths = glob.glob(path+'easy_ham/*')\n",
        "easy_ham_2_paths = glob.glob(path+'easy_ham_2/*')\n",
        "hard_ham_paths = glob.glob(path+'hard_ham/*')\n",
        "spam_paths = glob.glob(path+'spam/*')\n",
        "spam_2_paths = glob.glob(path+'spam_2/*')\n",
        "\n",
        "ham_path = [easy_ham_paths,easy_ham_2_paths,hard_ham_paths]\n",
        "\n",
        "spam_path = [spam_paths,spam_2_paths]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4RbluYtJg0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_email_content(email_path):\n",
        "    file = open(email_path,encoding='latin1')\n",
        "    try:\n",
        "        msg = email.message_from_file(file)\n",
        "        for part in msg.walk():\n",
        "            if part.get_content_type() == 'text/plain':\n",
        "                return part.get_payload() # prints the raw text\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        \n",
        "        \n",
        "def get_email_content_bulk(email_paths):\n",
        "    email_contents = [get_email_content(email_path) for email_path in email_paths]\n",
        "    return email_contents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJt4krb0Jg0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ham_sample = np.array([train_test_split(o) for o in ham_path])\n",
        "ham_train = np.array([])\n",
        "ham_test = np.array([])\n",
        "for o in ham_sample:\n",
        "    ham_train = np.concatenate((ham_train,o[0]),axis=0)\n",
        "    ham_test = np.concatenate((ham_test,o[1]),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJXnebpdJg0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spam_sample = np.array([train_test_split(o) for o in spam_path])\n",
        "spam_train = np.array([])\n",
        "spam_test = np.array([])\n",
        "for o in spam_sample:\n",
        "    spam_train = np.concatenate((spam_train,o[0]),axis=0)\n",
        "    spam_test = np.concatenate((spam_test,o[1]),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5-FPweOJg06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ham_train_label = [0]*ham_train.shape[0]\n",
        "spam_train_label = [1]*spam_train.shape[0]\n",
        "x_train = np.concatenate((ham_train,spam_train))\n",
        "y_train = np.concatenate((ham_train_label,spam_train_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkVocQJTJg0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ham_test_label = [0]*ham_test.shape[0]\n",
        "spam_test_label = [1]*spam_test.shape[0]\n",
        "x_test = np.concatenate((ham_test,spam_test))\n",
        "y_test = np.concatenate((ham_test_label,spam_test_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCWVWIByJg1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_shuffle_index = np.random.permutation(np.arange(0,x_train.shape[0]))\n",
        "test_shuffle_index = np.random.permutation(np.arange(0,x_test.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2sjP41_Jg1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train[train_shuffle_index]\n",
        "y_train = y_train[train_shuffle_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwAqxlTRJg1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = x_test[test_shuffle_index]\n",
        "y_test = y_test[test_shuffle_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyuS6Vl0Jg1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = get_email_content_bulk(x_train)\n",
        "x_test = get_email_content_bulk(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5ICGxkSJg1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_null(datas,labels):\n",
        "    not_null_idx = [i for i,o in enumerate(datas) if o is not None]\n",
        "    return np.array(datas)[not_null_idx],np.array(labels)[not_null_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPIhk_zkJg1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,y_train = remove_null(x_train,y_train)\n",
        "x_test,y_test = remove_null(x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKqYziutJg1d",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMg4m6JiJg1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Clean data by removing unnecessary characters and converting to lowercase ##\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "\n",
        "def remove_hyperlink(word):\n",
        "    return  re.sub(r\"http\\S+\", \"\", word)\n",
        "\n",
        "def to_lower(word):\n",
        "    result = word.lower()\n",
        "    return result\n",
        "\n",
        "def remove_number(word):\n",
        "    result = re.sub(r'\\d+', '', word)\n",
        "    return result\n",
        "\n",
        "def remove_punctuation(word):\n",
        "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
        "    return result\n",
        "\n",
        "def remove_whitespace(word):\n",
        "    result = word.strip()\n",
        "    return result\n",
        "\n",
        "def replace_newline(word):\n",
        "    return word.replace('\\n','')\n",
        "\n",
        "def clean_up_pipeline(sentence):\n",
        "    cleaning_utils = [remove_hyperlink,\n",
        "                      replace_newline,\n",
        "                      to_lower,\n",
        "                      remove_number,\n",
        "                      remove_punctuation,remove_whitespace]\n",
        "    for task in cleaning_utils:\n",
        "        sentence = task(sentence)\n",
        "    return sentence\n",
        "\n",
        "x_train = [clean_up_pipeline(sentence) for sentence in x_train]\n",
        "x_test = [clean_up_pipeline(sentence) for sentence in x_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTj_mB9hJg17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Tokenize, lemmatize and stem the data, and remove stopwords ##\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "x_train = [word_tokenize(sentence) for sentence in x_train]\n",
        "x_test = [word_tokenize(sentence) for sentence in x_test]\n",
        "\n",
        "\n",
        "def remove_stop_words(tokenized_sentence):\n",
        "    result = [word for word in tokenized_sentence if word not in ENGLISH_STOP_WORDS]\n",
        "    return result\n",
        "\n",
        "def word_stemmer(tokenized_sentence):\n",
        "    return [stemmer.stem(word) for word in tokenized_sentence]\n",
        "\n",
        "def word_lemmatizer(tokenized_sentence):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokenized_sentence]\n",
        "\n",
        "def clean_token_pipeline(tokenized_sentence):\n",
        "    cleaning_utils = [remove_stop_words,word_stemmer,word_lemmatizer]\n",
        "    for task in cleaning_utils:\n",
        "        preprocessed_sentence = task(tokenized_sentence)\n",
        "    return preprocessed_sentence\n",
        "\n",
        "x_train = [clean_token_pipeline(tokenized_sentence) for tokenized_sentence in x_train]\n",
        "x_test = [clean_token_pipeline(tokenized_sentence) for tokenized_sentence in x_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cIzc5Laoma7",
        "colab_type": "text"
      },
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb4fGxa-ossO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## This function is required to show the plotly graph in colab ##\n",
        "\n",
        "def configure_plotly_browser_state():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdsqVHibo7r3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objs as go\n",
        "from plotly import tools\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "\n",
        "\n",
        "x_train_join = [\" \".join(o) for o in x_train]\n",
        "x_test_join = [\" \".join(o) for o in x_test]\n",
        "\n",
        "spam_train_index = [i for i,o in enumerate(y_train) if o == 1]\n",
        "non_spam_train_index = [i for i,o in enumerate(y_train) if o == 0]\n",
        "\n",
        "spam_email = np.array(x_train_join)[spam_train_index]\n",
        "non_spam_email = np.array(x_train_join)[non_spam_train_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbB0hwTWApoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## N-gram bar chart visualization ##\n",
        "\n",
        "## custom function for ngram generation ##\n",
        "def generate_ngrams(text, n_gram=1):\n",
        "    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n",
        "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
        "    return [\" \".join(ngram) for ngram in ngrams]\n",
        "\n",
        "## custom function for horizontal bar chart ##\n",
        "def horizontal_bar_chart(df, color):\n",
        "    trace = go.Bar(\n",
        "        y=df[\"word\"].values[::-1],\n",
        "        x=df[\"wordcount\"].values[::-1],\n",
        "        showlegend=False,\n",
        "        orientation = 'h',\n",
        "        marker=dict(\n",
        "            color=color,\n",
        "        ),\n",
        "    )\n",
        "    return trace\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgnRQGssBABx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_in_bar_chart(word_count=1):\n",
        "    ## Get the bar chart from non-spam email ##\n",
        "    freq_dict = defaultdict(int)\n",
        "    for sent in non_spam_email:\n",
        "        for word in generate_ngrams(sent,word_count):\n",
        "            freq_dict[word] += 1\n",
        "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
        "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
        "    trace0 = horizontal_bar_chart(fd_sorted.head(20), 'orange')\n",
        "\n",
        "    ## Get the bar chart from spam email ##\n",
        "    freq_dict = defaultdict(int)\n",
        "    for sent in spam_email:\n",
        "        for word in generate_ngrams(sent,word_count):\n",
        "            freq_dict[word] += 1\n",
        "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
        "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
        "    trace1 = horizontal_bar_chart(fd_sorted.head(20), 'orange')\n",
        "\n",
        "    # Creating two subplots\n",
        "    fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n",
        "                              subplot_titles=[\"Frequent words of non spam email\", \n",
        "                                              \"Frequent words of spam email\"])\n",
        "    fig.append_trace(trace0, 1, 1)\n",
        "    fig.append_trace(trace1, 1, 2)\n",
        "    fig['layout'].update(height=600, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n",
        "    py.iplot(fig, filename='word-plots')\n",
        "\n",
        "\n",
        "configure_plotly_browser_state()\n",
        "visualize_in_bar_chart(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-bC1GKvqQlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Wordcloud Visualization ##\n",
        "\n",
        "def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n",
        "                   title = None, title_size=40, image_color=False):\n",
        "    stopwords = set(STOPWORDS)\n",
        "    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n",
        "    stopwords = stopwords.union(more_stopwords)\n",
        "\n",
        "    wordcloud = WordCloud(background_color='black',\n",
        "                    stopwords = stopwords,\n",
        "                    max_words = max_words,\n",
        "                    max_font_size = max_font_size, \n",
        "                    random_state = 42,\n",
        "                    width=800, \n",
        "                    height=400,\n",
        "                    mask = mask)\n",
        "    wordcloud.generate(str(text))\n",
        "    \n",
        "    plt.figure(figsize=figure_size)\n",
        "    if image_color:\n",
        "        image_colors = ImageColorGenerator(mask);\n",
        "        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n",
        "        plt.title(title, fontdict={'size': title_size,  \n",
        "                                  'verticalalignment': 'bottom'})\n",
        "    else:\n",
        "        plt.imshow(wordcloud);\n",
        "        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n",
        "                                  'verticalalignment': 'bottom'})\n",
        "    plt.axis('off');\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "#plot_wordcloud(spam_email,title = 'Spam Email')\n",
        "#plot_wordcloud(non_spam_email,title=\"Non Spam Email\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNlLm11DRVuG",
        "colab_type": "text"
      },
      "source": [
        "# TF-IDF based Embedding With  Naive-Bayes and SVM Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-cdLjfORq-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Compute TF-IDF features using sklearn ##\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "vectorizer.fit_transform(x_train_join)\n",
        "x_train_features = vectorizer.transform(x_train_join)\n",
        "x_test_features = vectorizer.transform(x_test_join)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50z3mJ5FWBD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Train a Naive Bayes Classifier ##\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = GaussianNB()\n",
        "\n",
        "clf.fit(x_train_features.toarray(),y_train)\n",
        "\n",
        "clf.score(x_test_features.toarray(),y_test)\n",
        "\n",
        "clf.score(x_train_features.toarray(),y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAwA2Jn3OLTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Train an SVM based Classifier ##\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "\n",
        "clf = SVC(gamma='auto')\n",
        "\n",
        "clf.fit(x_train_features.toarray(),y_train)\n",
        "\n",
        "clf.score(x_test_features.toarray(),y_test)\n",
        "\n",
        "clf.score(x_train_features.toarray(),y_train)\n",
        "\n",
        "clf_disp = plot_roc_curve(clf, x_test_features.toarray(),y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxfQbab_Jg2c",
        "colab_type": "text"
      },
      "source": [
        "# GloVe-based Embeding and Training using Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsHRosFGJg2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBwaw0D6Jg2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 2000\n",
        "max_features = 50000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v4x8c3GJg2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_FILE = 'glove.6B.300d.txt'\n",
        "tokenizer = Tokenizer(num_words=max_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOlphmFrJg2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFgjFkSRJg2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_features_dl = np.array(tokenizer.texts_to_sequences(x_train))\n",
        "x_test_features_dl = np.array(tokenizer.texts_to_sequences(x_test))\n",
        "\n",
        "x_train_features_dl = pad_sequences(x_train_features_dl,maxlen=maxlen)\n",
        "x_test_features_dl = pad_sequences(x_test_features_dl,maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crmVKUQNJg2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_coefs(word,*arr): \n",
        "  return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
        "\n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9ZXFPTgJg2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "        \n",
        "inp = Input(shape=(maxlen,))\n",
        "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
        "x = Bidirectional(LSTM(64, activation='tanh',return_sequences=True))(x)\n",
        "x = GlobalMaxPool1D()(x)\n",
        "x = Dense(16, activation=\"relu\")(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(1, activation=\"sigmoid\")(x)\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZYY8gJmJg20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.layers[1].trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hon62hmIJg21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x_train_features_dl,y_train, batch_size=512, epochs=20, \n",
        "          validation_data=(x_test_features_dl, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkSl2rJxJg23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOoYojWtJg3O",
        "colab_type": "text"
      },
      "source": [
        "# Custom Embedding and training using Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zncorqyvJg3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9W9PJZDJg3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## some config values \n",
        "embed_size = 100 # how big is each word vector\n",
        "max_feature = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "max_len = 2000 # max number of words in a question to use"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfrdY0NbJg3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=max_feature)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ulmgu2_Jg3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15_Olth-Jg3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_features_dl = np.array(tokenizer.texts_to_sequences(x_train))\n",
        "x_test_features_dl = np.array(tokenizer.texts_to_sequences(x_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STmnfjEsJg3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_features_dl = pad_sequences(x_train_features_dl,maxlen=max_len)\n",
        "x_test_features_dl = pad_sequences(x_test_features_dl,maxlen=max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxXSia3XJg3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = 100\n",
        "\n",
        "inp = Input(shape=(max_len,))\n",
        "x = Embedding(max_feature, embed_size)(inp)\n",
        "x = Bidirectional(LSTM(64, activation = 'tanh', return_sequences=True))(x)\n",
        "x = GlobalMaxPool1D()(x)\n",
        "x = Dense(16, activation=\"relu\")(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(1, activation=\"sigmoid\")(x)\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLR9OTUGJg3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x_train_features_dl, y_train, batch_size=512, epochs=20, validation_data=(x_test_features_dl, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6ddZ8E1Jg3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5yDxKs7Jg3l",
        "colab_type": "text"
      },
      "source": [
        "## Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzGF_hA4Jg3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix,f1_score, precision_score,recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "\n",
        "y_predict  = [1 if o>0.5 else 0 for o in model.predict(x_test_features)]\n",
        "\n",
        "confusion_matrix(y_test,y_predict)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_test,y_predict).ravel()\n",
        "\n",
        "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_predict)))\n",
        "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_predict)))\n",
        "\n",
        "f1_score(y_test,y_predict)\n",
        "\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test,y_predict)\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=['Non Spam','Spam'], normalize=False,\n",
        "                      title='Confusion matrix')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}